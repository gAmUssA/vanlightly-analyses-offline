In the [last](https://jack-vanlightly.com/analyses/2023/5/15/kafka-vs-
redpanda-performance-part-5-reaching-the-limits-of-the-nvme-drive) post we saw
how only Apache Kafka was able to fully utilize the 2 GB/s throughput limit of
the i3en.6xlarge. In this post we’re going to test the ability of Kafka and
Redpanda to drain a backlog while under continued producer load.

This test starts up the producers and consumers, at the target throughput, but
then pauses the consumers for a period until consumer lag builds up to a
desired amount (also known as a backlog), then the consumers are resumed and
we see how long it takes for them to catch-up and return to sub-second end-to-
end latency.

This test is an important one when doing a sizing analysis as this is an event
that can happen in production. Some kind of outage affects your consumers but
you don’t want to (or can’t) stop your producers. The ability of your
consumers to drain the backlog that has built up and return to normal is
critical.

## Measuring lag

See [Redpanda docs for measuring consumer
lag](https://docs.redpanda.com/docs/manage/monitoring/#consumer-group-lag).
Alternatively you can simply measure the client end-to-end metrics on the
clients dashboard of my OMB repository.

For Kafka, you can deploy the[ Kafka Lag
Exporter](https://github.com/seglo/kafka-lag-exporter) or simply use end-to-
end metrics as a guide.

# Noteworthy behavior

The Redpanda backlog draining tests I ran all had a common pattern of
behavior. When the brokers were in the early phase of backlog draining, the
brokers read from disk as those records were no longer cached in memory, but
when they could the brokers switched to the read caches in the final stage. In
every case, the reading from disk period was the most rapid and once reads to
disk subsided the backlog draining slowed. In some cases, as soon as reads
were being served from caches, the drain rate slowed so much that the backlog
started to grow again. In one case Redpanda entered an equilibrium where the
backlog partially drained and then remained fixed at a certain size.

# 1 GB/s producer load, 5TB backlog, consumer fanout of 1

As usual we’re using the three i3en.6xlarge according to the Redpanda
benchmarks and TCO.

In this test the consumers are down for 84 minutes which creates a 5TB backlog
on disk.

**Redpanda run #1 - After 4 hours, it was unable to drain the backlog**

The first run was not making progress after 4 hours.

View fullsize

![](https://images.squarespace-
cdn.com/content/v1/56894e581c1210fead06f878/4ac3387f-6e1d-4f83-b5c3-2dd24aa8142f/rp-
backlog-1GB-run1-labelled.png)

Fig 1. The backlog is not draining after 4 hours.

During the accelerated period of disk reading the Redpanda consumers were
unable to catch-up, so I figured that once the slower catch-up period started,
the backlog would start growing again - therefore I stopped the test at the 4
hour mark.

View fullsize

![](https://images.squarespace-
cdn.com/content/v1/56894e581c1210fead06f878/8ec52916-41ed-45d0-ac41-704d234289e9/backlog-
rp-1GB-disk.png)

Fig 2. When Redpanda reads from disk, the catch-up or drain rate is more
rapid.

**Redpanda run #2 - Unable to drain.**

After 9 hours, Redpanda reached an equilibrium where the backlog remained
partially drained with end-to-end latency being stable at 30 minutes.

View fullsize

![](https://images.squarespace-
cdn.com/content/v1/56894e581c1210fead06f878/9112311a-ed86-4801-838f-43de9942758b/rp-
backlog-1GB-run2-labelled.png)

Fig 3. Redpanda reaches an equilibrium where the backlog becomes stable but
never drains.

Catch-up is accelerated during read from disk phase. Once reads are served
from caches, the catchup slowed and Redpanda settled into the strange
equilibrium.

View fullsize

![](https://images.squarespace-
cdn.com/content/v1/56894e581c1210fead06f878/92e8b98a-6716-44e5-9697-00c5d2eda528/backlog-
rp-1GB-run2-disk.png)

Fig 4. The early read to disk phase subsides, and so too does the catch-up
rate.

**Kafka run #1 - Drained in 3 hours**

View fullsize

![](https://images.squarespace-
cdn.com/content/v1/56894e581c1210fead06f878/0bd64c60-1293-4722-ad0f-5200401afa9e/backlog-
kafka-1GB.png)

Fig 5. Kafka drains the backlog in 3 hours. We see consumer lag build up, then
starts to drop almost as quick as it grew. Kafka also saw a slow down in
catch-up towards the end here.

# 1 GB/s producer load, 2.5TB backlog drain test, fanout 2

Consumer downtime of 42 minutes creates a 2.5TB backlog on disk. With two
consumer groups, an extra 5TB of extra reads performed. Measure time for
backlog to reach zero while still receiving 1GB/s ingress.

 **Redpanda run - Backlog immediately grows - unable to drain.**

View fullsize

![](https://images.squarespace-
cdn.com/content/v1/56894e581c1210fead06f878/d32f7ed7-f6fa-4be6-b04d-8517dc6e009d/rp-
backlog-1GB-fanout2-labelled.png)

Fig 6. Redpanda didn’t get close to being able to drain the backlog with two
consumer groups and the constant 1 GB/s producer load.

**Kafka - Drained in 130 minutes**

View fullsize

![](https://images.squarespace-
cdn.com/content/v1/56894e581c1210fead06f878/f55b6ed7-3e78-4d60-83a2-ce19debb9236/backlog-
kafka-1GB-fanout2.png)

Fig 7. Kafka drains the backlog in 130 minutes.

# 800 MB/s, 1.67 TB backlog drain test, fanout 3

Consumer downtime of 34 minutes creates a 1.67TB backlog on disk. With three
consumer groups, an extra 5TB of extra reads performed. Measure time for
backlog to reach zero while still receiving 1GB/s ingress.

 **Redpanda - Backlog drops then grows - unable to drain**

View fullsize

![](https://images.squarespace-
cdn.com/content/v1/56894e581c1210fead06f878/88e94ef9-b914-48c5-b577-f81b7b857170/rp-
backlog-800MB-labelled.png)

Fig 8. At 800 MB/s, Redpanda initially does well, but then stalls and the
backlog starts to grow again.

**Kafka - drained in 74 minutes**

View fullsize

![](https://images.squarespace-
cdn.com/content/v1/56894e581c1210fead06f878/ddd30fd2-63f6-47b9-b7e8-d838fd66aecb/backlog-
kafka-800MB.png)

Fig 9. With constant 800 MB/s producer load, Kafka drains the backlog in 74
minutes.

# Conclusions

The ability for consumers to catch up after downtime, while producers continue
to apply load, is important for any event streaming system. Redpanda showed
that under the 800 MB/s and 1 GB/s workloads in this post, consumers were
unable to catch-up after a brief interruption. Kafka on the other hand was
able to drain the backlogs and return to normal in all cases.

Backlog draining tests are an important part of a sizing analysis and could be
overlooked. It seems this test was overlooked when the Redpanda TCO analysis
was performed.

# How to run this test

To run a backlog draining test, simply add the following line to your workload
file:

 _consumerBacklogSizeGB: 5000_

The above line tells OMB to pause consumers until 500 GB of consumer lag has
accumulated. It counts each subscription in the calculation so 5TB with 3
subscriptions means that 1.67TB of backlog is built up on disk.

You can track the consumer lag of Redpanda with a [PromQL
query](https://docs.redpanda.com/docs/manage/monitoring/#consumer-group-lag).

For Kafka, view the [Kafka Lag Exporter Github
repo](https://github.com/seglo/kafka-lag-exporter) for instructions.

You can see the workloads I ran in my OMB repo
[here](https://github.com/Vanlightly/openmessaging-benchmark-
custom/tree/main/blog/kafka-vs-rp/part6-draining-backlogs).

Series links:

  * [Kafka vs Redpanda performance - Do the claims add up?](https://jack-vanlightly.com/blog/2023/5/15/kafka-vs-redpanda-performance-do-the-claims-add-up)

  * [Kafka vs Redpanda performance - Part 1 - 4 vs 50 producers](https://jack-vanlightly.com/analyses/2023/5/15/kafka-vs-redpanda-performance-part-1-4-vs-50-producers)

  * [Kafka vs Redpanda performance - Part 2 - Long running tests](https://jack-vanlightly.com/analyses/2023/5/15/kafka-vs-redpanda-performance-part-2-long-running-tests)

  * [Kafka vs Redpanda performance - Part 3 - Hitting the retention limit](https://jack-vanlightly.com/analyses/2023/5/15/kafka-vs-redpanda-performance-part-3-hitting-the-retention-limit)

  * [Kafka vs Redpanda performance - Part 4 - The impact of record keys](https://jack-vanlightly.com/analyses/2023/5/15/kafka-vs-redpanda-performance-part-4-impact-of-record-keys)

  * [Kafka vs Redpanda performance - Part 5 - Reaching the NVMe drive limit](https://jack-vanlightly.com/analyses/2023/5/15/kafka-vs-redpanda-performance-part-5-reaching-the-limits-of-the-nvme-drive)

  * Kafka vs Redpanda performance - Part 6 - Draining backlogs

